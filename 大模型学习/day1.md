一、核心信息概览

1、模型基础
    名称: DeepSeek V3 LLM 通用模型   概率续写
    架构: MoE（混合专家模型）
    算法: 强化学习
    训练方式: 并行训练

2、性能表现
    R1 推理模型: 671B 参数规模
    对标对象: 比肩 OpenAI o1 模型
    特性: 涌现能力、泛化能力强

3、技术亮点
    Scaling Law 应用
        通过数据、参数、算力的同步提升来增强模型能力
        体现了大规模语言模型的发展规律

4、实际应用场景
    4.1企业本地部署，如常规文本生成任务、提高工作效率。
        (1)r1
        (2)量化版
        (3)蒸馏版
            模型压缩技术
            知识迁移（能力迁移）
    4.2 私有化业务流程部署
        幻觉：
         rag
            问题 → 大模型 base模型 → 答案
            问题 → 私有知识库（查询） → Prompt（查询结果 + 问题） → 大模型 → 生成答案      
        工程化
            私有知识库构建
            在预算足够的情况下，选择参数大一些的模型效果会更好！ 
         fine tuning(微调)
            数据集大小，细分领域
            训练过程中，GPU成本问题，越大GPU要求越高
            多少条数据就可以有效果？1.5B —— 100条左右
            有标注的高质量数据（数据工程）
                （1）覆盖尽量多得到业务场景
                （2）数据尽量均衡
                （3）数据真实，从业务中提取数据，做好标记、打好标签
